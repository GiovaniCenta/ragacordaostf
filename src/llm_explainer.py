"""
LLM Explainer â€“ Qwen QwQ-32B-AWQ (4-bit, fp16)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ AtÃ© 32 k tokens de contexto; atÃ© 10 k tokens de resposta.
â€¢ Preserva o bloco <think> completo â€“ sem cortes.
â€¢ Parser lÃª o primeiro â€œCorreto/Correta/Incorreto/Incorretaâ€ encontrado
  e devolve o restante como justificativa.

DependÃªncias:
  pip install -U autoawq transformers accelerate safetensors \
      "awq @ https://huggingface.github.io/autawq-wheels/cu121"
"""
from __future__ import annotations

import gc
import logging
import re
from typing import List, Optional, Tuple

import torch
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - EXPLAINER - %(levelname)s - %(message)s",
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ constants
MODEL_ID   = "Qwen/QwQ-32B-AWQ"
DTYPE      = torch.float16       # evita mismatch fp16 Ã— bf16
MAX_CTX_TK = 32_000

_model: Optional[AutoAWQForCausalLM] = None
_tok:   Optional[AutoTokenizer]      = None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ model loader
def _load_model() -> Tuple[AutoAWQForCausalLM, AutoTokenizer]:
    global _model, _tok
    if _model is not None:
        return _model, _tok

    gc.collect()
    torch.cuda.empty_cache()
    logging.info(f"ğŸ”„ Carregando {MODEL_ID}â€¦")

    _model = AutoAWQForCausalLM.from_quantized(
        MODEL_ID,
        device_map="auto",
        torch_dtype=DTYPE,
        fuse_layers=True,
    ).eval()

    _tok = AutoTokenizer.from_pretrained(MODEL_ID)
    if _tok.pad_token is None:
        _tok.pad_token = _tok.eos_token
        _tok.padding_side = "right"

    logging.info("âœ… Modelo carregado.")
    return _model, _tok


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ prompt builder
def _build_prompt(statement: str, paragraphs: List[str]) -> str:
    ctx = "\n\n".join(
        f"ParÃ¡grafo {i+1}:\n{p}" for i, p in enumerate(paragraphs) if p.strip()
    ) or "Nenhum parÃ¡grafo de contexto foi fornecido."

    system_msg = (
        "VocÃª Ã© um avaliador jurÃ­dico. Determine se a AFIRMAÃ‡ÃƒO estÃ¡ correta "
        "Ã  luz dos PARÃGRAFOS.\n\n"
        "CRITÃ‰RIOS:\n"
        "â€¢ 'Incorreto' â†’ contradiÃ§Ã£o factual OU falta/divergÃªncia de fato essencial.\n"
        "â€¢ 'Correto'   â†’ todos os fatos essenciais presentes ou parÃ¡frase fiel.\n\n"
        "FORMATO OBRIGATÃ“RIO:\n"
        "Correto.\nJustificativa: â€¦\n\nou\n\n"
        "Incorreto.\nJustificativa: â€¦\n\n"
        "Comece **exatamente** com 'Correto.' ou 'Incorreto.' (sem 'Resultado:', "
        "sem Markdown). Responda em portuguÃªs."
    )

    user_msg = f"AFIRMAÃ‡ÃƒO: {statement}\n\nPARÃGRAFOS:\n{ctx}"

    # Formato chat Qwen-2
    return (
        "<|im_start|>system\n" + system_msg + "\n<|im_end|>\n"
        "<|im_start|>user\n"   + user_msg   + "\n<|im_end|>\n"
        "<|im_start|>assistant\n"
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main API
def explain(
    query: str,
    list_of_texts: List[str],
    *,
    max_new_tokens: int = 10_000,
    temperature: float = 0.0,
) -> Tuple[str, str]:
    """
    Retorna (veredito, justificativa).
    â€¢ Nenhum critÃ©rio de parada explÃ­cito â€’ sÃ³ â€˜max_new_tokensâ€™.
    â€¢ Inteiro <think> preservado.
    """
    if not query or not list_of_texts:
        return "ERRO_DE_PARSING", "Dados de entrada ausentes para o LLM."

    try:
        model, tok = _load_model()
        device = next(model.parameters()).device

        prompt = _build_prompt(query, list_of_texts)
        inp = tok(prompt, return_tensors="pt", truncation=True, padding=False)
        if inp.input_ids.shape[1] >= MAX_CTX_TK:
            return "ERRO_DE_PARSING", "Prompt excede o limite de contexto."

        inp = {k: v.to(device) for k, v in inp.items()}

        gen_ids = model.generate(
            **inp,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=0.9,
            do_sample=temperature > 0,
            pad_token_id=tok.pad_token_id,  # â† Ãºnico token especial mantido
            # **eos_token_id REMOVIDO**  (nenhum critÃ©rio de parada)
        )

        reply = tok.decode(
            gen_ids[0, inp["input_ids"].shape[1]:],
            skip_special_tokens=True,
        ).strip()

        # remove cabeÃ§alho â€œParÃ¡grafo X:â€ caso escape
        reply = re.sub(r"^ParÃ¡grafo[s]?\s*\d+(?:-?\d+)?[:.]?\s*", "", reply, flags=re.I).strip()

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ parser
        m = re.search(r"(?:Resultado\s*:\s*)?\s*(Corret[oa]|Incorret[oa])\b\.?", reply, re.I)
        if not m:
            return "ERRO_DE_PARSING", f"LLM nÃ£o seguiu o formato esperado. Resposta bruta: {reply}"

        verdict = "Correto" if m.group(1).lower().startswith("corret") else "Incorreto"
        rationale_raw = reply[m.end():].lstrip()

        j = re.search(r"Justificativa\s*:\s*(.*)", rationale_raw, re.I | re.S)
        rationale = j.group(1).strip() if j else rationale_raw or "Justificativa ausente."

        return verdict, rationale

    except Exception as exc:  # noqa: BLE001
        logging.exception("Erro em explain()")
        return "ERRO_DE_PARSING", f"Falha na geraÃ§Ã£o da explicaÃ§Ã£o pelo LLM: {exc}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ quick test
if __name__ == "__main__":
    v, r = explain(
        "Segundo o TCU, o banco nÃ£o Ã© dependente da UniÃ£o.",
        ["SUMÃRIO: REPRESENTAÃ‡ÃƒO â€¦ NÃƒO ENQUADRAMENTO DO BNDES â€¦"],
    )
    print("VEREDITO:", v)
    print("JUSTIFICATIVA:\n", r)
