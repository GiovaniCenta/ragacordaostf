import pandas as pd
import argparse
import os
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    precision_recall_fscore_support
)
import logging

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - EVAL - %(levelname)s - %(message)s')

def evaluate_predictions(results_file, ground_truth_file, output_error_file="evaluation_errors.csv"):
    """
    Compares prediction results against ground truth and calculates metrics.

    Args:
        results_file (str): Path to the CSV file generated by main.py (e.g., 'resultados.csv').
        ground_truth_file (str): Path to the ground truth CSV file.
        output_error_file (str): Path to save the CSV file with misclassified sentences.
    """
    logging.info(f"Iniciando avaliação...")
    logging.info(f"Arquivo de Resultados: {results_file}")
    logging.info(f"Arquivo Ground Truth: {ground_truth_file}")

    # --- 1. Load Data ---
    try:
        # Specify delimiter and handle potential quote issues if Sentenca has quotes
        results_df = pd.read_csv(results_file, delimiter=';', quotechar='"')
        ground_truth_df = pd.read_csv(ground_truth_file, delimiter=';', quotechar='"')
        logging.info(f"Arquivos carregados. Resultados: {len(results_df)} linhas, Ground Truth: {len(ground_truth_df)} linhas.")
    except FileNotFoundError as e:
        logging.error(f"Erro: Arquivo não encontrado - {e}")
        return
    except Exception as e:
        logging.error(f"Erro ao ler arquivos CSV: {e}")
        return

    # --- 2. Data Validation and Merging ---
    required_results_cols = ['ID', 'Sentenca', 'Prediction']
    required_gt_cols = ['ID', 'Sentenca', 'Predicao'] # Assuming ground truth uses 'Predicao'

    if not all(col in results_df.columns for col in required_results_cols):
        logging.error(f"Erro: Colunas esperadas ausentes no arquivo de resultados: {required_results_cols}")
        return
    if not all(col in ground_truth_df.columns for col in required_gt_cols):
        logging.error(f"Erro: Colunas esperadas ausentes no arquivo ground truth: {required_gt_cols}")
        return

    # Merge based on ID - using inner merge to only evaluate matching IDs
    try:
        # Ensure ID is treated as the same type if needed (e.g., string or int)
        results_df['ID'] = results_df['ID'].astype(int)
        ground_truth_df['ID'] = ground_truth_df['ID'].astype(int)
        
        # Select relevant columns before merge to avoid duplicate 'Sentenca'
        gt_relevant = ground_truth_df[['ID', 'Predicao']]
        
        merged_df = pd.merge(results_df, gt_relevant, on='ID', how='inner')
        logging.info(f"DataFrames combinados por ID. Total de {len(merged_df)} sentenças correspondentes.")

        if len(merged_df) == 0:
             logging.error("Erro: Nenhum ID correspondente encontrado entre os arquivos de resultados e ground truth.")
             return
             
        # Optional: Verify sentence text matches for the merged IDs (can help catch ID mismatches)
        # merged_check_df = pd.merge(results_df[['ID', 'Sentenca']], ground_truth_df[['ID', 'Sentenca']], on='ID', suffixes=('_res', '_gt'))
        # mismatches = merged_check_df[merged_check_df['Sentenca_res'] != merged_check_df['Sentenca_gt']]
        # if not mismatches.empty:
        #     logging.warning(f"Aviso: {len(mismatches)} IDs têm textos de sentença diferentes entre os arquivos. Verifique os IDs: {mismatches['ID'].tolist()}")

    except Exception as e:
        logging.error(f"Erro durante a combinação dos dados: {e}")
        return

    # --- 3. Prepare Labels for Metrics ---
    # Ensure case consistency (e.g., convert both to lowercase)
    y_true = merged_df['Predicao'].str.lower().str.strip()
    y_pred = merged_df['Prediction'].str.lower().str.strip()

    # Get the unique labels present in the ground truth
    labels = sorted(y_true.unique())
    logging.info(f"Classes encontradas no Ground Truth: {labels}")

    # --- 4. Calculate Quantitative Metrics ---
    print("\n--- Avaliação Quantitativa ---")

    # Accuracy
    accuracy = accuracy_score(y_true, y_pred)
    print(f"\nAcurácia Geral: {accuracy:.4f}")

    # Classification Report (Precision, Recall, F1-Score per class)
    print("\nRelatório de Classificação:")
    # Use labels found in y_true to handle cases where model didn't predict a certain class
    report = classification_report(y_true, y_pred, labels=labels, zero_division=0)
    print(report)

    # Confusion Matrix
    print("\nMatriz de Confusão:")
    # Rows = True Labels, Columns = Predicted Labels
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    # Print with labels for clarity
    print(f"Labels: {labels}")
    print(cm)

    # --- 5. Prepare Qualitative Analysis Data (Errors) ---
    print(f"\n--- Preparando Dados para Análise Qualitativa ---")

    # Find misclassified rows
    errors_df = merged_df[y_true != y_pred].copy() # Use .copy() to avoid SettingWithCopyWarning

    # Select and rename columns for the error report
    errors_df = errors_df[['ID', 'Sentenca', 'Prediction', 'Predicao', 'Confianca']] # Added Confianca
    errors_df.rename(columns={'Predicao': 'GroundTruth', 'Prediction': 'PredicaoSistema', 'Confianca': 'ConfiancaSistema'}, inplace=True)

    if errors_df.empty:
        logging.info("Nenhum erro de classificação encontrado!")
        print("\nNenhum erro de classificação encontrado!")
    else:
        logging.info(f"Encontrados {len(errors_df)} erros de classificação.")
        print(f"\nEncontrados {len(errors_df)} erros de classificação.")
        try:
            # Save errors to CSV
            errors_df.to_csv(output_error_file, sep=';', index=False, encoding='utf-8', quotechar='"', quoting=csv.QUOTE_NONNUMERIC)
            print(f"Relatório de erros salvo em: {output_error_file}")
            logging.info(f"Relatório de erros salvo em: {output_error_file}")
        except Exception as e:
            logging.error(f"Falha ao salvar relatório de erros em CSV: {e}")
            print(f"Falha ao salvar relatório de erros em CSV: {e}")

    print("\nNota: Para análise qualitativa completa dos erros, consulte as justificativas geradas pelo 'main.py'")
    print(f"(seja na saída do console ou no arquivo {os.path.splitext(results_file)[0]}_validation_results.txt)")
    print("Use o 'ID' do arquivo de erros para encontrar a justificativa correspondente.")

    logging.info("Avaliação concluída.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Avalia os resultados da validação de sentenças contra um gabarito (ground truth).")
    parser.add_argument("results_csv", help="Caminho para o arquivo CSV de resultados gerado pelo main.py.")
    parser.add_argument("ground_truth_csv", help="Caminho para o arquivo CSV de ground truth.")
    parser.add_argument("-o", "--output_errors", default="evaluation_errors.csv",
                        help="Nome do arquivo para salvar os erros de classificação (default: evaluation_errors.csv)")

    args = parser.parse_args()

    evaluate_predictions(args.results_csv, args.ground_truth_csv, args.output_errors)
