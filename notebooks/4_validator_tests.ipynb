{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding f:\\interview\\acordao\\acordao_validator to sys.path\n",
      "Basic setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup, Path, and Imports\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import unittest\n",
    "from unittest.mock import patch, MagicMock # For mocking LLM calls later if desired\n",
    "\n",
    "# Configure logging for tests\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - VALIDATOR_TEST - %(levelname)s - %(message)s')\n",
    "\n",
    "# Add the 'src' directory to the Python path to find the modules\n",
    "module_path = os.path.abspath(os.path.join('..')) # Assumes notebook is in 'notebooks/' dir\n",
    "if module_path not in sys.path:\n",
    "    print(f\"Adding {module_path} to sys.path\")\n",
    "    sys.path.append(module_path)\n",
    "else:\n",
    "    print(f\"{module_path} already in sys.path\")\n",
    "\n",
    "print(\"Basic setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported from validator.\n",
      "Successfully imported torch and transformers.\n",
      "Resetting validator module's cached model/tokenizer instances for testing.\n",
      "Imports and constant setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and Constants\n",
    "\n",
    "# Standard library imports already done\n",
    "\n",
    "# --- Project Modules ---\n",
    "try:\n",
    "    from src.validator import (\n",
    "        load_llm_model_and_tokenizer,\n",
    "        _format_validation_prompt,\n",
    "        validate_claim_with_llm,\n",
    "        LLM_MODEL_ID # Import the constant used\n",
    "    )\n",
    "    print(\"Successfully imported from validator.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR importing from validator: {e}\")\n",
    "    # Define fallbacks to avoid NameErrors\n",
    "    load_llm_model_and_tokenizer = None\n",
    "    _format_validation_prompt = None\n",
    "    validate_claim_with_llm = None\n",
    "    LLM_MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\" # Fallback\n",
    "\n",
    "# --- Third-party ---\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    print(\"Successfully imported torch and transformers.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR importing torch/transformers: {e}\")\n",
    "    torch = None\n",
    "    AutoModelForCausalLM = None\n",
    "    AutoTokenizer = None\n",
    "\n",
    "# --- Reset Caches (Important for testing loading) ---\n",
    "# Access the cached variables in the validator module if it exists\n",
    "if 'src.validator' in sys.modules:\n",
    "    print(\"Resetting validator module's cached model/tokenizer instances for testing.\")\n",
    "    sys.modules['src.validator']._llm_model_instance = None\n",
    "    sys.modules['src.validator']._llm_tokenizer_instance = None\n",
    "\n",
    "print(\"Imports and constant setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 16:32:14,811 - VALIDATOR_TEST - INFO - Loading LLM model and tokenizer for: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-04-30 16:32:14,814 - VALIDATOR_TEST - INFO - Loading model with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Running Test: test_llm_loading ==========\n",
      "\n",
      "--- Test Case 1: Successful Loading ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 16:32:19,530 - VALIDATOR_TEST - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:54<00:00, 18.33s/it]\n",
      "2025-04-30 16:33:14,923 - VALIDATOR_TEST - INFO - Model loaded successfully.\n",
      "2025-04-30 16:33:14,924 - VALIDATOR_TEST - INFO - Loading tokenizer...\n",
      "2025-04-30 16:33:15,251 - VALIDATOR_TEST - INFO - Set tokenizer pad_token to eos_token and padding_side to left.\n",
      "2025-04-30 16:33:15,252 - VALIDATOR_TEST - INFO - Tokenizer loaded successfully.\n",
      "2025-04-30 16:33:15,255 - VALIDATOR_TEST - INFO - GPU Memory after load: Allocated=3.84 GB, Reserved=4.07 GB\n",
      "2025-04-30 16:33:15,258 - VALIDATOR_TEST - INFO - Loading LLM model and tokenizer for: invalid/model-does-not-exist-at-all-hopefully\n",
      "2025-04-30 16:33:15,262 - VALIDATOR_TEST - INFO - Loading model with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n",
      "Tokenizer type: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
      "Model loaded on device: cuda:0\n",
      "-> Test Case 1 PASSED (Loaded in 60.45s)\n",
      "\n",
      "--- Test Case 2: Caching ---\n",
      "Calling load function again...\n",
      "Second call returned objects in 0.0000s (should be much faster).\n",
      "-> Test Case 2 PASSED (Caching appears functional)\n",
      "\n",
      "--- Test Case 3: Invalid Model ID ---\n",
      "Attempting to load invalid model: invalid/model-does-not-exist-at-all-hopefully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 16:33:15,449 - VALIDATOR_TEST - ERROR - Failed to load LLM model or tokenizer: invalid/model-does-not-exist-at-all-hopefully is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 409, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/invalid/model-does-not-exist-at-all-hopefully/resolve/main/config.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 424, in cached_files\n",
      "    hf_hub_download(\n",
      "    ~~~~~~~~~~~~~~~^\n",
      "        path_or_repo_id,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    ...<10 lines>...\n",
      "        local_files_only=local_files_only,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 961, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "        # Destination\n",
      "    ...<14 lines>...\n",
      "        force_download=force_download,\n",
      "    )\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1068, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1596, in _raise_on_head_call_error\n",
      "    raise head_call_error\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1484, in _get_metadata_or_catch_error\n",
      "    metadata = get_hf_file_metadata(\n",
      "        url=url, proxies=proxies, timeout=etag_timeout, headers=headers, token=token\n",
      "    )\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1401, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "        method=\"HEAD\",\n",
      "    ...<5 lines>...\n",
      "        timeout=timeout,\n",
      "    )\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 285, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "        method=method,\n",
      "    ...<2 lines>...\n",
      "        **params,\n",
      "    )\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 309, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 459, in hf_raise_for_status\n",
      "    raise _format(RepositoryNotFoundError, message, response) from e\n",
      "huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-68127afb-65c94f797eb4f20856974628;1de4d3f1-f4d0-40b3-a81c-539d0d308387)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/invalid/model-does-not-exist-at-all-hopefully/resolve/main/config.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\src\\validator.py\", line 56, in load_llm_model_and_tokenizer\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "        model_id,\n",
      "    ...<3 lines>...\n",
      "        low_cpu_mem_usage=True,\n",
      "    )\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 492, in from_pretrained\n",
      "    resolved_config_file = cached_file(\n",
      "        pretrained_model_name_or_path,\n",
      "    ...<4 lines>...\n",
      "        **hub_kwargs,\n",
      "    )\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 266, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 456, in cached_files\n",
      "    raise OSError(\n",
      "    ...<4 lines>...\n",
      "    ) from e\n",
      "OSError: invalid/model-does-not-exist-at-all-hopefully is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught expected RuntimeError: Failed to load LLM model or tokenizer: invalid/model-does-not-exist-at-all-hopefully is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "-> Test Case 3 PASSED (Correctly raised error for invalid ID)\n",
      "\n",
      "========== End of Test: test_llm_loading ==========\n",
      "Result: ALL test_llm_loading tests PASSED\n",
      "Resetting validator module's cached model/tokenizer instances after tests.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test load_llm_model_and_tokenizer\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" Running Test: test_llm_loading \" + \"=\"*10)\n",
    "\n",
    "# --- Test Case 1: Successful Loading ---\n",
    "print(\"\\n--- Test Case 1: Successful Loading ---\")\n",
    "model_instance = None\n",
    "tokenizer_instance = None\n",
    "test_passed_1 = False\n",
    "load_time = 0\n",
    "if load_llm_model_and_tokenizer is None or torch is None or not torch.cuda.is_available():\n",
    "     print(\"SKIPPING Test Case 1: Validator/Torch import failed or CUDA not available.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        model_instance, tokenizer_instance = load_llm_model_and_tokenizer() # Use default model ID\n",
    "        load_time = time.time() - start_time\n",
    "\n",
    "        assert model_instance is not None, \"Model should be loaded\"\n",
    "        assert tokenizer_instance is not None, \"Tokenizer should be loaded\"\n",
    "        # Check types (might be wrapped in specific classes by transformers)\n",
    "        # assert isinstance(model_instance, AutoModelForCausalLM) # Type might vary slightly\n",
    "        # assert isinstance(tokenizer_instance, AutoTokenizer) # Type might vary slightly\n",
    "        print(f\"Model type: {type(model_instance)}\")\n",
    "        print(f\"Tokenizer type: {type(tokenizer_instance)}\")\n",
    "        print(f\"Model loaded on device: {model_instance.device}\")\n",
    "        # Basic check if it's on GPU\n",
    "        assert 'cuda' in str(model_instance.device), \"Model should be on CUDA device\"\n",
    "\n",
    "        test_passed_1 = True\n",
    "        print(f\"-> Test Case 1 PASSED (Loaded in {load_time:.2f}s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"-> Test Case 1 FAILED: {e}\")\n",
    "\n",
    "\n",
    "# --- Test Case 2: Caching ---\n",
    "# Requires Test Case 1 to have passed and loaded the model\n",
    "print(\"\\n--- Test Case 2: Caching ---\")\n",
    "test_passed_2 = False\n",
    "if not test_passed_1:\n",
    "     print(\"SKIPPING Test Case 2: Requires successful load in Test Case 1.\")\n",
    "elif load_llm_model_and_tokenizer is None:\n",
    "     print(\"SKIPPING Test Case 2: Validator function not imported.\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"Calling load function again...\")\n",
    "        start_time = time.time()\n",
    "        model_cached, tokenizer_cached = load_llm_model_and_tokenizer()\n",
    "        cache_load_time = time.time() - start_time\n",
    "\n",
    "        # Check if the SAME objects were returned (caching worked)\n",
    "        assert model_cached is model_instance, \"Cached model object should be the same instance\"\n",
    "        assert tokenizer_cached is tokenizer_instance, \"Cached tokenizer object should be the same instance\"\n",
    "        print(f\"Second call returned objects in {cache_load_time:.4f}s (should be much faster).\")\n",
    "        assert cache_load_time < 1.0, \"Cache loading should be very fast (e.g., < 1 second)\"\n",
    "\n",
    "        test_passed_2 = True\n",
    "        print(\"-> Test Case 2 PASSED (Caching appears functional)\")\n",
    "    except Exception as e:\n",
    "        print(f\"-> Test Case 2 FAILED: {e}\")\n",
    "\n",
    "# --- Test Case 3: Invalid Model ID ---\n",
    "print(\"\\n--- Test Case 3: Invalid Model ID ---\")\n",
    "test_passed_3 = False\n",
    "if load_llm_model_and_tokenizer is None:\n",
    "     print(\"SKIPPING Test Case 3: Validator function not imported.\")\n",
    "else:\n",
    "    # Reset cache before testing failure\n",
    "    if 'src.validator' in sys.modules:\n",
    "        sys.modules['src.validator']._llm_model_instance = None\n",
    "        sys.modules['src.validator']._llm_tokenizer_instance = None\n",
    "\n",
    "    invalid_model_id = \"invalid/model-does-not-exist-at-all-hopefully\"\n",
    "    print(f\"Attempting to load invalid model: {invalid_model_id}\")\n",
    "    try:\n",
    "        load_llm_model_and_tokenizer(model_id=invalid_model_id)\n",
    "        print(\"-> Test Case 3 FAILED: Loading invalid model should have raised an error.\")\n",
    "    except RuntimeError as e:\n",
    "        # We expect a RuntimeError wrapping the underlying Hugging Face error\n",
    "        print(f\"Caught expected RuntimeError: {e}\")\n",
    "        test_passed_3 = True\n",
    "        print(\"-> Test Case 3 PASSED (Correctly raised error for invalid ID)\")\n",
    "    except Exception as e:\n",
    "         print(f\"-> Test Case 3 FAILED: Caught unexpected error type {type(e).__name__}: {e}\")\n",
    "\n",
    "# Final Result for this Cell\n",
    "print(\"\\n\" + \"=\"*10 + \" End of Test: test_llm_loading \" + \"=\"*10)\n",
    "if test_passed_1 and test_passed_2 and test_passed_3:\n",
    "    print(\"Result: ALL test_llm_loading tests PASSED\")\n",
    "else:\n",
    "    print(\"Result: SOME test_llm_loading tests FAILED\")\n",
    "\n",
    "# Reset cache again after tests\n",
    "if 'src.validator' in sys.modules:\n",
    "    print(\"Resetting validator module's cached model/tokenizer instances after tests.\")\n",
    "    sys.modules['src.validator']._llm_model_instance = None\n",
    "    sys.modules['src.validator']._llm_tokenizer_instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Running Test: test_prompt_formatting ==========\n",
      "Prompt Preview:\n",
      "--------------------\n",
      "<s>[INST] Você é um assistente especialista em analisar documentos jurídicos do TCU. Sua tarefa é avaliar se uma alegação feita em um resumo é verdadeira ou falsa, baseando-se *estritamente* nos trechos fornecidos do documento original.\n",
      "\n",
      "**Instruções:**\n",
      "1. Analise a 'Alegação' abaixo.\n",
      "2. Verifique se a alegação pode ser confirmada ou refutada usando *apenas* as informações contidas nos 'Trechos do Documento Original'. Não use conhecimento externo.\n",
      "3. Responda no seguinte formato EXATO:\n",
      "   Resultado: [Correta/Incorreta]\n",
      "   Justificativa: [Explique brevemente o motivo com base nos trechos, ou 'N/A' se for Correta]\n",
      "\n",
      "**Trechos do Documento Original:**\n",
      "Trecho 1:\n",
      "Contexto número um.\n",
      "\n",
      "Trecho 2:\n",
      "Contexto número dois com\n",
      "quebra de linha.\n",
      "\n",
      "**Alegação a ser validada:**\n",
      "Alegação de teste. [/INST]\n",
      "--------------------\n",
      "-> Test PASSED\n",
      "\n",
      "========== End of Test: test_prompt_formatting ==========\n",
      "Result: test_prompt_formatting PASSED\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test _format_validation_prompt\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" Running Test: test_prompt_formatting \" + \"=\"*10)\n",
    "\n",
    "test_passed = False\n",
    "if _format_validation_prompt is None:\n",
    "    print(\"SKIPPING test_prompt_formatting: Function not imported.\")\n",
    "else:\n",
    "    try:\n",
    "        sample_claim = \"Alegação de teste.\"\n",
    "        sample_chunks = [\"Contexto número um.\", \"Contexto número dois com\\nquebra de linha.\"]\n",
    "\n",
    "        # This is the expected formatting of the context *within* the user message\n",
    "        expected_context_str = \"Trecho 1:\\nContexto número um.\\n\\nTrecho 2:\\nContexto número dois com\\nquebra de linha.\"\n",
    "\n",
    "        prompt = _format_validation_prompt(sample_claim, sample_chunks)\n",
    "\n",
    "        # Check 1: Basic structure for Mistral Instruct\n",
    "        assert prompt.startswith(\"<s>[INST]\"), \"Prompt should start with <s>[INST]\"\n",
    "        assert prompt.endswith(\"[/INST]\"), \"Prompt should end with [/INST]\"\n",
    "        # Check it doesn't contain the old markers accidentally\n",
    "        assert \"<|user|>\" not in prompt, \"Old Phi-3 markers should not be present\"\n",
    "        assert \"<|assistant|>\" not in prompt, \"Old Phi-3 markers should not be present\"\n",
    "\n",
    "        # Check 2: Claim is present within the user message part\n",
    "        # Need to check between the [INST] tags\n",
    "        user_message_part = prompt[len(\"<s>[INST]\"): -len(\" [/INST]\")].strip() # Extract content\n",
    "        assert f\"**Alegação a ser validada:**\\n{sample_claim}\" in user_message_part, \"Claim not found in user message\"\n",
    "\n",
    "        # Check 3: Context is present and formatted within the user message part\n",
    "        assert f\"**Trechos do Documento Original:**\\n{expected_context_str}\" in user_message_part, \"Formatted context not found in user message\"\n",
    "\n",
    "        # Check 4: Instructions are present within the user message part\n",
    "        assert \"Resultado: [Correta/Incorreta]\" in user_message_part, \"Output format instructions missing\"\n",
    "        assert \"Justificativa: [Explique brevemente\" in user_message_part, \"Output format instructions missing\"\n",
    "\n",
    "        print(\"Prompt Preview:\\n\" + \"-\"*20 + f\"\\n{prompt}\\n\" + \"-\"*20)\n",
    "        test_passed = True\n",
    "        print(\"-> Test PASSED\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "         print(f\"-> Test FAILED: Assertion Error: {e}\")\n",
    "    except Exception as e:\n",
    "         print(f\"-> Test FAILED: Unexpected Error: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" End of Test: test_prompt_formatting \" + \"=\"*10)\n",
    "if test_passed:\n",
    "    print(\"Result: test_prompt_formatting PASSED\")\n",
    "else:\n",
    "    print(\"Result: test_prompt_formatting FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 16:34:59,053 - VALIDATOR_TEST - INFO - Loading LLM model and tokenizer for: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-04-30 16:34:59,055 - VALIDATOR_TEST - INFO - Loading model with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Running Test: test_llm_validation (Integration) ==========\n",
      "\n",
      "--- Subtest 1: Validating Correct Claim ---\n",
      "Claim: O BNDES é uma estatal dependente da União\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 16:34:59,426 - VALIDATOR_TEST - INFO - Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 327680000.0 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n",
      "2025-04-30 16:34:59,426 - VALIDATOR_TEST - ERROR - Failed to load LLM model or tokenizer: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\src\\validator.py\", line 56, in load_llm_model_and_tokenizer\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "        model_id,\n",
      "    ...<3 lines>...\n",
      "        low_cpu_mem_usage=True,\n",
      "    )\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 571, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 279, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4380, in from_pretrained\n",
      "    device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 1304, in _get_device_map\n",
      "    hf_quantizer.validate_environment(device_map=device_map)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py\", line 104, in validate_environment\n",
      "    raise ValueError(\n",
      "    ...<6 lines>...\n",
      "    )\n",
      "ValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "2025-04-30 16:34:59,429 - VALIDATOR_TEST - ERROR - Runtime error during LLM validation (possibly OOM or model loading failed): Failed to load LLM model or tokenizer: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "2025-04-30 16:34:59,431 - VALIDATOR_TEST - INFO - Loading LLM model and tokenizer for: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-04-30 16:34:59,433 - VALIDATOR_TEST - INFO - Loading model with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Subtest 1 FAILED: Assertion Error: Validation failed, returned None\n",
      "\n",
      "--- Subtest 2: Validating Incorrect Claim ---\n",
      "Claim: O BNDES opera apenas com recursos próprios.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 16:34:59,876 - VALIDATOR_TEST - INFO - Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 327680000.0 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n",
      "2025-04-30 16:34:59,877 - VALIDATOR_TEST - ERROR - Failed to load LLM model or tokenizer: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\src\\validator.py\", line 56, in load_llm_model_and_tokenizer\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "        model_id,\n",
      "    ...<3 lines>...\n",
      "        low_cpu_mem_usage=True,\n",
      "    )\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 571, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 279, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4380, in from_pretrained\n",
      "    device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 1304, in _get_device_map\n",
      "    hf_quantizer.validate_environment(device_map=device_map)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py\", line 104, in validate_environment\n",
      "    raise ValueError(\n",
      "    ...<6 lines>...\n",
      "    )\n",
      "ValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "2025-04-30 16:34:59,879 - VALIDATOR_TEST - ERROR - Runtime error during LLM validation (possibly OOM or model loading failed): Failed to load LLM model or tokenizer: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Subtest 2 FAILED: Assertion Error: Validation failed, returned None\n",
      "\n",
      "========== End of Test: test_llm_validation (Integration) ==========\n",
      "Result: test_llm_validation FAILED\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Test validate_claim_with_llm (Integration Style)\n",
    "# NOTE: This cell runs the ACTUAL LLM. It requires the model to be loaded\n",
    "# (which might happen here for the first time if Cell 3 was skipped or failed)\n",
    "# and requires a functioning GPU with sufficient VRAM.\n",
    "# It will be SLOW the first time it loads the model.\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" Running Test: test_llm_validation (Integration) \" + \"=\"*10)\n",
    "\n",
    "test_passed = False\n",
    "# --- Dummy Data (Simulating retriever output) ---\n",
    "# Use the same dummy data as the validator's main block for consistency\n",
    "sample_claim_valid = \"O BNDES é uma estatal dependente da União\"\n",
    "sample_context_valid = {\n",
    "    'ids': [['doc_id_1', 'doc_id_2']],\n",
    "    'documents': [[\n",
    "        \"Trecho 1: O Banco Nacional de Desenvolvimento Econômico e Social (BNDES) é uma empresa pública federal...\",\n",
    "        \"Trecho 2: Conforme análise do Ministério da Fazenda, o BNDES se enquadra como empresa estatal dependente, sujeita ao teto remuneratório.\"\n",
    "    ]],\n",
    "    'metadatas': [[{'source': 'doc.pdf', 'page': 1}, {'source': 'doc.pdf', 'page': 5}]],\n",
    "    'distances': [[0.1, 0.2]]\n",
    "}\n",
    "\n",
    "sample_claim_invalid = \"O BNDES opera apenas com recursos próprios.\"\n",
    "sample_context_invalid = { # Same context for simplicity\n",
    "    'ids': [['doc_id_1', 'doc_id_2']],\n",
    "    'documents': [[\n",
    "        \"Trecho 1: O banco utiliza recursos públicos, como os do FAT e FMM, embora parte dos recursos também provenha de captações próprias.\",\n",
    "        \"Trecho 2: A estrutura de financiamento do BNDES inclui fontes do Tesouro Nacional e fundos governamentais.\"\n",
    "    ]],\n",
    "    'metadatas': [[{'source': 'doc.pdf', 'page': 10}, {'source': 'doc.pdf', 'page': 11}]],\n",
    "    'distances': [[0.15, 0.25]]\n",
    "}\n",
    "\n",
    "\n",
    "if validate_claim_with_llm is None or torch is None or not torch.cuda.is_available():\n",
    "    print(\"SKIPPING test_llm_validation: Validator/Torch import failed or CUDA not available.\")\n",
    "else:\n",
    "    all_subtests_passed = True\n",
    "    validation_result_valid = None\n",
    "    validation_result_invalid = None\n",
    "    try:\n",
    "        # --- Subtest 1: Claim expected to be Correct ---\n",
    "        print(\"\\n--- Subtest 1: Validating Correct Claim ---\")\n",
    "        print(f\"Claim: {sample_claim_valid}\")\n",
    "        validation_result_valid = validate_claim_with_llm(sample_claim_valid, sample_context_valid)\n",
    "\n",
    "        assert validation_result_valid is not None, \"Validation failed, returned None\"\n",
    "        assert isinstance(validation_result_valid, dict), \"Result should be a dict\"\n",
    "        assert \"Resultado\" in validation_result_valid, \"Result dict missing 'Resultado'\"\n",
    "        assert \"Justificativa\" in validation_result_valid, \"Result dict missing 'Justificativa'\"\n",
    "        print(f\"Parsed Result 1: {validation_result_valid}\")\n",
    "        # We expect the LLM (if working well) to classify this as Correta based on context\n",
    "        assert validation_result_valid[\"Resultado\"] == \"Correta\", f\"Expected 'Correta', got '{validation_result_valid['Resultado']}'\"\n",
    "        print(\"-> Subtest 1 PASSED (Structurally)\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "        print(f\"-> Subtest 1 FAILED: Assertion Error: {e}\")\n",
    "        all_subtests_passed = False\n",
    "    except Exception as e:\n",
    "        print(f\"-> Subtest 1 FAILED: Unexpected Error: {e}\")\n",
    "        all_subtests_passed = False\n",
    "\n",
    "\n",
    "    try:\n",
    "        # --- Subtest 2: Claim expected to be Incorrect ---\n",
    "        print(\"\\n--- Subtest 2: Validating Incorrect Claim ---\")\n",
    "        print(f\"Claim: {sample_claim_invalid}\")\n",
    "        validation_result_invalid = validate_claim_with_llm(sample_claim_invalid, sample_context_invalid)\n",
    "\n",
    "        assert validation_result_invalid is not None, \"Validation failed, returned None\"\n",
    "        assert isinstance(validation_result_invalid, dict), \"Result should be a dict\"\n",
    "        assert \"Resultado\" in validation_result_invalid, \"Result dict missing 'Resultado'\"\n",
    "        assert \"Justificativa\" in validation_result_invalid, \"Result dict missing 'Justificativa'\"\n",
    "        print(f\"Parsed Result 2: {validation_result_invalid}\")\n",
    "        # We expect the LLM to classify this as Incorreta\n",
    "        assert validation_result_invalid[\"Resultado\"] == \"Incorreta\", f\"Expected 'Incorreta', got '{validation_result_invalid['Resultado']}'\"\n",
    "        # Justification should not be empty or N/A for incorrect claims\n",
    "        assert validation_result_invalid[\"Justificativa\"] != \"N/A\", \"Justificativa should not be 'N/A' for incorrect claim\"\n",
    "        assert len(validation_result_invalid[\"Justificativa\"]) > 5, \"Justificativa seems too short\"\n",
    "        print(\"-> Subtest 2 PASSED (Structurally)\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "        print(f\"-> Subtest 2 FAILED: Assertion Error: {e}\")\n",
    "        all_subtests_passed = False\n",
    "    except Exception as e:\n",
    "        print(f\"-> Subtest 2 FAILED: Unexpected Error: {e}\")\n",
    "        all_subtests_passed = False\n",
    "\n",
    "    # Set overall test pass status\n",
    "    test_passed = all_subtests_passed\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" End of Test: test_llm_validation (Integration) \" + \"=\"*10)\n",
    "if test_passed:\n",
    "    print(\"Result: test_llm_validation PASSED\")\n",
    "else:\n",
    "    print(\"Result: test_llm_validation FAILED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 16:35:36,289 - VALIDATOR_TEST - ERROR - An unexpected error occurred during LLM validation: 'dict' object has no attribute 'to'\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\src\\validator.py\", line 181, in validate_claim_with_llm\n",
      "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=False, truncation=False).to(model.device)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'dict' object has no attribute 'to'\n",
      "2025-04-30 16:35:36,361 - VALIDATOR_TEST - ERROR - An unexpected error occurred during LLM validation: 'dict' object has no attribute 'to'\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\src\\validator.py\", line 181, in validate_claim_with_llm\n",
      "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=False, truncation=False).to(model.device)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'dict' object has no attribute 'to'\n",
      "2025-04-30 16:35:36,363 - VALIDATOR_TEST - ERROR - An unexpected error occurred during LLM validation: 'dict' object has no attribute 'to'\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\interview\\acordao\\acordao_validator\\src\\validator.py\", line 181, in validate_claim_with_llm\n",
      "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=False, truncation=False).to(model.device)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'dict' object has no attribute 'to'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Running Test: test_llm_parsing (Mocked) ==========\n",
      "\n",
      "--- Subtest 1: Mocking Correct Response ---\n",
      "-> Subtest 1 FAILED: Parsing failed for Correct case, got None\n",
      "\n",
      "--- Subtest 2: Mocking Incorrect Response ---\n",
      "-> Subtest 2 FAILED: Parsing failed for Incorrect case, got None\n",
      "\n",
      "--- Subtest 3: Mocking Malformed Response ---\n",
      "-> Subtest 3 FAILED: Parsing failed for Malformed case, got None\n",
      "\n",
      "========== End of Test: test_llm_parsing (Mocked) ==========\n",
      "Result: test_llm_parsing FAILED\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test validate_claim_with_llm Parsing Logic (Mocked)\n",
    "# This tests the parsing part of the validator without running the actual LLM.\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" Running Test: test_llm_parsing (Mocked) \" + \"=\"*10)\n",
    "\n",
    "# Dummy claim/context needed for function call structure\n",
    "dummy_claim = \"Test claim for parsing.\"\n",
    "dummy_context = { 'documents': [['Dummy context document.']] }\n",
    "\n",
    "# --- Mocking Setup ---\n",
    "# We need to mock the behavior *within* validate_claim_with_llm\n",
    "# Specifically: load_llm_model_and_tokenizer and model.generate\n",
    "\n",
    "# Mock model and tokenizer objects\n",
    "mock_model = MagicMock()\n",
    "mock_tokenizer = MagicMock()\n",
    "mock_tokenizer.decode = MagicMock() # Mock the decode method\n",
    "mock_tokenizer.eos_token_id = 123 # Example ID\n",
    "mock_tokenizer.pad_token_id = 123\n",
    "\n",
    "# Mock the input tokenization result (only shape is usually needed for slicing)\n",
    "mock_inputs = {'input_ids': torch.tensor([[1, 2, 3]])} # Dummy tensor\n",
    "mock_tokenizer.return_value = mock_inputs # Mock the tokenizer call\n",
    "\n",
    "# Mock the output tensor from model.generate\n",
    "# Needs to have the input shape + generated shape\n",
    "# Shape: (batch_size, sequence_length)\n",
    "# Let's simulate generating 10 tokens after the 3 input tokens\n",
    "mock_output_ids = torch.tensor([[1, 2, 3, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
    "mock_model.generate.return_value = mock_output_ids\n",
    "\n",
    "test_passed = False\n",
    "if validate_claim_with_llm is None:\n",
    "     print(\"SKIPPING test_llm_parsing: Validator function not imported.\")\n",
    "else:\n",
    "    all_mock_tests_passed = True\n",
    "    # --- Subtest 1: Mock Correct Response ---\n",
    "    print(\"\\n--- Subtest 1: Mocking Correct Response ---\")\n",
    "    # Define what tokenizer.decode should return for the generated part\n",
    "    mock_response_correct = \"Resultado: Correta\\nJustificativa: N/A\"\n",
    "    mock_tokenizer.decode.return_value = mock_response_correct\n",
    "    try:\n",
    "        with patch('src.validator.load_llm_model_and_tokenizer', return_value=(mock_model, mock_tokenizer)):\n",
    "             # The patch replaces the function temporarily within this block\n",
    "             result_correct = validate_claim_with_llm(dummy_claim, dummy_context)\n",
    "\n",
    "        assert result_correct == {\"Resultado\": \"Correta\", \"Justificativa\": \"N/A\"}, f\"Parsing failed for Correct case, got {result_correct}\"\n",
    "        print(f\"Parsed Result: {result_correct}\")\n",
    "        print(\"-> Subtest 1 PASSED\")\n",
    "    except Exception as e:\n",
    "         print(f\"-> Subtest 1 FAILED: {e}\")\n",
    "         all_mock_tests_passed = False\n",
    "\n",
    "\n",
    "    # --- Subtest 2: Mock Incorrect Response ---\n",
    "    print(\"\\n--- Subtest 2: Mocking Incorrect Response ---\")\n",
    "    mock_response_incorrect = \"Resultado: Incorreta\\nJustificativa: O trecho 1 contradiz a alegação.\"\n",
    "    mock_tokenizer.decode.return_value = mock_response_incorrect\n",
    "    try:\n",
    "        with patch('src.validator.load_llm_model_and_tokenizer', return_value=(mock_model, mock_tokenizer)):\n",
    "             result_incorrect = validate_claim_with_llm(dummy_claim, dummy_context)\n",
    "\n",
    "        expected_incorrect = {\"Resultado\": \"Incorreta\", \"Justificativa\": \"O trecho 1 contradiz a alegação.\"}\n",
    "        assert result_incorrect == expected_incorrect, f\"Parsing failed for Incorrect case, got {result_incorrect}\"\n",
    "        print(f\"Parsed Result: {result_incorrect}\")\n",
    "        print(\"-> Subtest 2 PASSED\")\n",
    "    except Exception as e:\n",
    "         print(f\"-> Subtest 2 FAILED: {e}\")\n",
    "         all_mock_tests_passed = False\n",
    "\n",
    "\n",
    "    # --- Subtest 3: Mock Malformed Response ---\n",
    "    print(\"\\n--- Subtest 3: Mocking Malformed Response ---\")\n",
    "    mock_response_malformed = \"Uh oh, I forgot the format.\\nResultado : maybe correct\\nJustif: idk\"\n",
    "    mock_tokenizer.decode.return_value = mock_response_malformed\n",
    "    try:\n",
    "        with patch('src.validator.load_llm_model_and_tokenizer', return_value=(mock_model, mock_tokenizer)):\n",
    "             result_malformed = validate_claim_with_llm(dummy_claim, dummy_context)\n",
    "\n",
    "        # Expect the default error state\n",
    "        expected_malformed = {\"Resultado\": \"Erro\", \"Justificativa\": \"Falha ao parsear resposta do LLM.\"}\n",
    "        assert result_malformed == expected_malformed, f\"Parsing failed for Malformed case, got {result_malformed}\"\n",
    "        print(f\"Parsed Result: {result_malformed}\")\n",
    "        print(\"-> Subtest 3 PASSED\")\n",
    "    except Exception as e:\n",
    "         print(f\"-> Subtest 3 FAILED: {e}\")\n",
    "         all_mock_tests_passed = False\n",
    "\n",
    "    test_passed = all_mock_tests_passed\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" End of Test: test_llm_parsing (Mocked) \" + \"=\"*10)\n",
    "if test_passed:\n",
    "    print(\"Result: test_llm_parsing PASSED\")\n",
    "else:\n",
    "    print(\"Result: test_llm_parsing FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acordao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
