{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding f:\\interview\\acordao\\acordao_validator to sys.path\n",
      "Basic setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup, Path, and Imports\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import unittest\n",
    "from unittest.mock import patch, MagicMock # For mocking LLM calls later if desired\n",
    "\n",
    "# Configure logging for tests\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - VALIDATOR_TEST - %(levelname)s - %(message)s')\n",
    "\n",
    "# Add the 'src' directory to the Python path to find the modules\n",
    "module_path = os.path.abspath(os.path.join('..')) # Assumes notebook is in 'notebooks/' dir\n",
    "if module_path not in sys.path:\n",
    "    print(f\"Adding {module_path} to sys.path\")\n",
    "    sys.path.append(module_path)\n",
    "else:\n",
    "    print(f\"{module_path} already in sys.path\")\n",
    "\n",
    "print(\"Basic setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports and Constants\n",
    "\n",
    "# Standard library imports already done\n",
    "\n",
    "# --- Project Modules ---\n",
    "try:\n",
    "    from src.validator import (\n",
    "        load_llm_model_and_tokenizer,\n",
    "        _format_validation_prompt,\n",
    "        validate_claim_with_llm,\n",
    "        LLM_MODEL_ID # Import the constant used\n",
    "    )\n",
    "    print(\"Successfully imported from validator.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR importing from validator: {e}\")\n",
    "    # Define fallbacks to avoid NameErrors\n",
    "    load_llm_model_and_tokenizer = None\n",
    "    _format_validation_prompt = None\n",
    "    validate_claim_with_llm = None\n",
    "    LLM_MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\" # Fallback\n",
    "\n",
    "# --- Third-party ---\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    print(\"Successfully imported torch and transformers.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR importing torch/transformers: {e}\")\n",
    "    torch = None\n",
    "    AutoModelForCausalLM = None\n",
    "    AutoTokenizer = None\n",
    "\n",
    "# --- Reset Caches (Important for testing loading) ---\n",
    "# Access the cached variables in the validator module if it exists\n",
    "if 'src.validator' in sys.modules:\n",
    "    print(\"Resetting validator module's cached model/tokenizer instances for testing.\")\n",
    "    sys.modules['src.validator']._llm_model_instance = None\n",
    "    sys.modules['src.validator']._llm_tokenizer_instance = None\n",
    "\n",
    "print(\"Imports and constant setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Test load_llm_model_and_tokenizer\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" Running Test: test_llm_loading \" + \"=\"*10)\n",
    "\n",
    "# --- Test Case 1: Successful Loading ---\n",
    "print(\"\\n--- Test Case 1: Successful Loading ---\")\n",
    "model_instance = None\n",
    "tokenizer_instance = None\n",
    "test_passed_1 = False\n",
    "load_time = 0\n",
    "if load_llm_model_and_tokenizer is None or torch is None or not torch.cuda.is_available():\n",
    "     print(\"SKIPPING Test Case 1: Validator/Torch import failed or CUDA not available.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        model_instance, tokenizer_instance = load_llm_model_and_tokenizer() # Use default model ID\n",
    "        load_time = time.time() - start_time\n",
    "\n",
    "        assert model_instance is not None, \"Model should be loaded\"\n",
    "        assert tokenizer_instance is not None, \"Tokenizer should be loaded\"\n",
    "        # Check types (might be wrapped in specific classes by transformers)\n",
    "        # assert isinstance(model_instance, AutoModelForCausalLM) # Type might vary slightly\n",
    "        # assert isinstance(tokenizer_instance, AutoTokenizer) # Type might vary slightly\n",
    "        print(f\"Model type: {type(model_instance)}\")\n",
    "        print(f\"Tokenizer type: {type(tokenizer_instance)}\")\n",
    "        print(f\"Model loaded on device: {model_instance.device}\")\n",
    "        # Basic check if it's on GPU\n",
    "        assert 'cuda' in str(model_instance.device), \"Model should be on CUDA device\"\n",
    "\n",
    "        test_passed_1 = True\n",
    "        print(f\"-> Test Case 1 PASSED (Loaded in {load_time:.2f}s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"-> Test Case 1 FAILED: {e}\")\n",
    "\n",
    "\n",
    "# --- Test Case 2: Caching ---\n",
    "# Requires Test Case 1 to have passed and loaded the model\n",
    "print(\"\\n--- Test Case 2: Caching ---\")\n",
    "test_passed_2 = False\n",
    "if not test_passed_1:\n",
    "     print(\"SKIPPING Test Case 2: Requires successful load in Test Case 1.\")\n",
    "elif load_llm_model_and_tokenizer is None:\n",
    "     print(\"SKIPPING Test Case 2: Validator function not imported.\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"Calling load function again...\")\n",
    "        start_time = time.time()\n",
    "        model_cached, tokenizer_cached = load_llm_model_and_tokenizer()\n",
    "        cache_load_time = time.time() - start_time\n",
    "\n",
    "        # Check if the SAME objects were returned (caching worked)\n",
    "        assert model_cached is model_instance, \"Cached model object should be the same instance\"\n",
    "        assert tokenizer_cached is tokenizer_instance, \"Cached tokenizer object should be the same instance\"\n",
    "        print(f\"Second call returned objects in {cache_load_time:.4f}s (should be much faster).\")\n",
    "        assert cache_load_time < 1.0, \"Cache loading should be very fast (e.g., < 1 second)\"\n",
    "\n",
    "        test_passed_2 = True\n",
    "        print(\"-> Test Case 2 PASSED (Caching appears functional)\")\n",
    "    except Exception as e:\n",
    "        print(f\"-> Test Case 2 FAILED: {e}\")\n",
    "\n",
    "# --- Test Case 3: Invalid Model ID ---\n",
    "print(\"\\n--- Test Case 3: Invalid Model ID ---\")\n",
    "test_passed_3 = False\n",
    "if load_llm_model_and_tokenizer is None:\n",
    "     print(\"SKIPPING Test Case 3: Validator function not imported.\")\n",
    "else:\n",
    "    # Reset cache before testing failure\n",
    "    if 'src.validator' in sys.modules:\n",
    "        sys.modules['src.validator']._llm_model_instance = None\n",
    "        sys.modules['src.validator']._llm_tokenizer_instance = None\n",
    "\n",
    "    invalid_model_id = \"invalid/model-does-not-exist-at-all-hopefully\"\n",
    "    print(f\"Attempting to load invalid model: {invalid_model_id}\")\n",
    "    try:\n",
    "        load_llm_model_and_tokenizer(model_id=invalid_model_id)\n",
    "        print(\"-> Test Case 3 FAILED: Loading invalid model should have raised an error.\")\n",
    "    except RuntimeError as e:\n",
    "        # We expect a RuntimeError wrapping the underlying Hugging Face error\n",
    "        print(f\"Caught expected RuntimeError: {e}\")\n",
    "        test_passed_3 = True\n",
    "        print(\"-> Test Case 3 PASSED (Correctly raised error for invalid ID)\")\n",
    "    except Exception as e:\n",
    "         print(f\"-> Test Case 3 FAILED: Caught unexpected error type {type(e).__name__}: {e}\")\n",
    "\n",
    "# Final Result for this Cell\n",
    "print(\"\\n\" + \"=\"*10 + \" End of Test: test_llm_loading \" + \"=\"*10)\n",
    "if test_passed_1 and test_passed_2 and test_passed_3:\n",
    "    print(\"Result: ALL test_llm_loading tests PASSED\")\n",
    "else:\n",
    "    print(\"Result: SOME test_llm_loading tests FAILED\")\n",
    "\n",
    "# Reset cache again after tests\n",
    "if 'src.validator' in sys.modules:\n",
    "    print(\"Resetting validator module's cached model/tokenizer instances after tests.\")\n",
    "    sys.modules['src.validator']._llm_model_instance = None\n",
    "    sys.modules['src.validator']._llm_tokenizer_instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Test _format_validation_prompt\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" Running Test: test_prompt_formatting \" + \"=\"*10)\n",
    "\n",
    "test_passed = False\n",
    "if _format_validation_prompt is None:\n",
    "    print(\"SKIPPING test_prompt_formatting: Function not imported.\")\n",
    "else:\n",
    "    try:\n",
    "        sample_claim = \"Alegação de teste.\"\n",
    "        sample_chunks = [\"Contexto número um.\", \"Contexto número dois com\\nquebra de linha.\"]\n",
    "\n",
    "        expected_context_str = \"Trecho 1:\\nContexto número um.\\n\\nTrecho 2:\\nContexto número dois com\\nquebra de linha.\"\n",
    "\n",
    "        prompt = _format_validation_prompt(sample_claim, sample_chunks)\n",
    "\n",
    "        # Check 1: Basic structure and roles\n",
    "        assert prompt.startswith(\"<|user|>\"), \"Prompt should start with <|user|>\"\n",
    "        assert prompt.endswith(\"<|assistant|>\"), \"Prompt should end with <|assistant|>\"\n",
    "        assert \"<|end|>\\n<|assistant|>\" in prompt, \"Prompt structure missing <|end|> <|assistant|>\"\n",
    "\n",
    "        # Check 2: Claim is present\n",
    "        assert f\"**Alegação a ser validada:**\\n{sample_claim}\" in prompt, \"Claim not found in prompt\"\n",
    "\n",
    "        # Check 3: Context is present and formatted\n",
    "        assert f\"**Trechos do Documento Original:**\\n{expected_context_str}\" in prompt, \"Formatted context not found in prompt\"\n",
    "\n",
    "        # Check 4: Instructions are present\n",
    "        assert \"Resultado: [Correta/Incorreta]\" in prompt, \"Output format instructions missing\"\n",
    "        assert \"Justificativa: [Explique brevemente\" in prompt, \"Output format instructions missing\"\n",
    "\n",
    "        print(\"Prompt Preview:\\n\" + \"-\"*20 + f\"\\n{prompt}\\n\" + \"-\"*20)\n",
    "        test_passed = True\n",
    "        print(\"-> Test PASSED\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "         print(f\"-> Test FAILED: Assertion Error: {e}\")\n",
    "    except Exception as e:\n",
    "         print(f\"-> Test FAILED: Unexpected Error: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" End of Test: test_prompt_formatting \" + \"=\"*10)\n",
    "if test_passed:\n",
    "    print(\"Result: test_prompt_formatting PASSED\")\n",
    "else:\n",
    "    print(\"Result: test_prompt_formatting FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Test validate_claim_with_llm (Integration Style)\n",
    "# NOTE: This cell runs the ACTUAL LLM. It requires the model to be loaded\n",
    "# (which might happen here for the first time if Cell 3 was skipped or failed)\n",
    "# and requires a functioning GPU with sufficient VRAM.\n",
    "# It will be SLOW the first time it loads the model.\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" Running Test: test_llm_validation (Integration) \" + \"=\"*10)\n",
    "\n",
    "test_passed = False\n",
    "# --- Dummy Data (Simulating retriever output) ---\n",
    "# Use the same dummy data as the validator's main block for consistency\n",
    "sample_claim_valid = \"O BNDES é uma estatal dependente da União\"\n",
    "sample_context_valid = {\n",
    "    'ids': [['doc_id_1', 'doc_id_2']],\n",
    "    'documents': [[\n",
    "        \"Trecho 1: O Banco Nacional de Desenvolvimento Econômico e Social (BNDES) é uma empresa pública federal...\",\n",
    "        \"Trecho 2: Conforme análise do Ministério da Fazenda, o BNDES se enquadra como empresa estatal dependente, sujeita ao teto remuneratório.\"\n",
    "    ]],\n",
    "    'metadatas': [[{'source': 'doc.pdf', 'page': 1}, {'source': 'doc.pdf', 'page': 5}]],\n",
    "    'distances': [[0.1, 0.2]]\n",
    "}\n",
    "\n",
    "sample_claim_invalid = \"O BNDES opera apenas com recursos próprios.\"\n",
    "sample_context_invalid = { # Same context for simplicity\n",
    "    'ids': [['doc_id_1', 'doc_id_2']],\n",
    "    'documents': [[\n",
    "        \"Trecho 1: O banco utiliza recursos públicos, como os do FAT e FMM, embora parte dos recursos também provenha de captações próprias.\",\n",
    "        \"Trecho 2: A estrutura de financiamento do BNDES inclui fontes do Tesouro Nacional e fundos governamentais.\"\n",
    "    ]],\n",
    "    'metadatas': [[{'source': 'doc.pdf', 'page': 10}, {'source': 'doc.pdf', 'page': 11}]],\n",
    "    'distances': [[0.15, 0.25]]\n",
    "}\n",
    "\n",
    "\n",
    "if validate_claim_with_llm is None or torch is None or not torch.cuda.is_available():\n",
    "    print(\"SKIPPING test_llm_validation: Validator/Torch import failed or CUDA not available.\")\n",
    "else:\n",
    "    all_subtests_passed = True\n",
    "    validation_result_valid = None\n",
    "    validation_result_invalid = None\n",
    "    try:\n",
    "        # --- Subtest 1: Claim expected to be Correct ---\n",
    "        print(\"\\n--- Subtest 1: Validating Correct Claim ---\")\n",
    "        print(f\"Claim: {sample_claim_valid}\")\n",
    "        validation_result_valid = validate_claim_with_llm(sample_claim_valid, sample_context_valid)\n",
    "\n",
    "        assert validation_result_valid is not None, \"Validation failed, returned None\"\n",
    "        assert isinstance(validation_result_valid, dict), \"Result should be a dict\"\n",
    "        assert \"Resultado\" in validation_result_valid, \"Result dict missing 'Resultado'\"\n",
    "        assert \"Justificativa\" in validation_result_valid, \"Result dict missing 'Justificativa'\"\n",
    "        print(f\"Parsed Result 1: {validation_result_valid}\")\n",
    "        # We expect the LLM (if working well) to classify this as Correta based on context\n",
    "        assert validation_result_valid[\"Resultado\"] == \"Correta\", f\"Expected 'Correta', got '{validation_result_valid['Resultado']}'\"\n",
    "        print(\"-> Subtest 1 PASSED (Structurally)\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "        print(f\"-> Subtest 1 FAILED: Assertion Error: {e}\")\n",
    "        all_subtests_passed = False\n",
    "    except Exception as e:\n",
    "        print(f\"-> Subtest 1 FAILED: Unexpected Error: {e}\")\n",
    "        all_subtests_passed = False\n",
    "\n",
    "\n",
    "    try:\n",
    "        # --- Subtest 2: Claim expected to be Incorrect ---\n",
    "        print(\"\\n--- Subtest 2: Validating Incorrect Claim ---\")\n",
    "        print(f\"Claim: {sample_claim_invalid}\")\n",
    "        validation_result_invalid = validate_claim_with_llm(sample_claim_invalid, sample_context_invalid)\n",
    "\n",
    "        assert validation_result_invalid is not None, \"Validation failed, returned None\"\n",
    "        assert isinstance(validation_result_invalid, dict), \"Result should be a dict\"\n",
    "        assert \"Resultado\" in validation_result_invalid, \"Result dict missing 'Resultado'\"\n",
    "        assert \"Justificativa\" in validation_result_invalid, \"Result dict missing 'Justificativa'\"\n",
    "        print(f\"Parsed Result 2: {validation_result_invalid}\")\n",
    "        # We expect the LLM to classify this as Incorreta\n",
    "        assert validation_result_invalid[\"Resultado\"] == \"Incorreta\", f\"Expected 'Incorreta', got '{validation_result_invalid['Resultado']}'\"\n",
    "        # Justification should not be empty or N/A for incorrect claims\n",
    "        assert validation_result_invalid[\"Justificativa\"] != \"N/A\", \"Justificativa should not be 'N/A' for incorrect claim\"\n",
    "        assert len(validation_result_invalid[\"Justificativa\"]) > 5, \"Justificativa seems too short\"\n",
    "        print(\"-> Subtest 2 PASSED (Structurally)\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "        print(f\"-> Subtest 2 FAILED: Assertion Error: {e}\")\n",
    "        all_subtests_passed = False\n",
    "    except Exception as e:\n",
    "        print(f\"-> Subtest 2 FAILED: Unexpected Error: {e}\")\n",
    "        all_subtests_passed = False\n",
    "\n",
    "    # Set overall test pass status\n",
    "    test_passed = all_subtests_passed\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" End of Test: test_llm_validation (Integration) \" + \"=\"*10)\n",
    "if test_passed:\n",
    "    print(\"Result: test_llm_validation PASSED\")\n",
    "else:\n",
    "    print(\"Result: test_llm_validation FAILED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test validate_claim_with_llm Parsing Logic (Mocked)\n",
    "# This tests the parsing part of the validator without running the actual LLM.\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" Running Test: test_llm_parsing (Mocked) \" + \"=\"*10)\n",
    "\n",
    "# Dummy claim/context needed for function call structure\n",
    "dummy_claim = \"Test claim for parsing.\"\n",
    "dummy_context = { 'documents': [['Dummy context document.']] }\n",
    "\n",
    "# --- Mocking Setup ---\n",
    "# We need to mock the behavior *within* validate_claim_with_llm\n",
    "# Specifically: load_llm_model_and_tokenizer and model.generate\n",
    "\n",
    "# Mock model and tokenizer objects\n",
    "mock_model = MagicMock()\n",
    "mock_tokenizer = MagicMock()\n",
    "mock_tokenizer.decode = MagicMock() # Mock the decode method\n",
    "mock_tokenizer.eos_token_id = 123 # Example ID\n",
    "mock_tokenizer.pad_token_id = 123\n",
    "\n",
    "# Mock the input tokenization result (only shape is usually needed for slicing)\n",
    "mock_inputs = {'input_ids': torch.tensor([[1, 2, 3]])} # Dummy tensor\n",
    "mock_tokenizer.return_value = mock_inputs # Mock the tokenizer call\n",
    "\n",
    "# Mock the output tensor from model.generate\n",
    "# Needs to have the input shape + generated shape\n",
    "# Shape: (batch_size, sequence_length)\n",
    "# Let's simulate generating 10 tokens after the 3 input tokens\n",
    "mock_output_ids = torch.tensor([[1, 2, 3, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
    "mock_model.generate.return_value = mock_output_ids\n",
    "\n",
    "test_passed = False\n",
    "if validate_claim_with_llm is None:\n",
    "     print(\"SKIPPING test_llm_parsing: Validator function not imported.\")\n",
    "else:\n",
    "    all_mock_tests_passed = True\n",
    "    # --- Subtest 1: Mock Correct Response ---\n",
    "    print(\"\\n--- Subtest 1: Mocking Correct Response ---\")\n",
    "    # Define what tokenizer.decode should return for the generated part\n",
    "    mock_response_correct = \"Resultado: Correta\\nJustificativa: N/A\"\n",
    "    mock_tokenizer.decode.return_value = mock_response_correct\n",
    "    try:\n",
    "        with patch('src.validator.load_llm_model_and_tokenizer', return_value=(mock_model, mock_tokenizer)):\n",
    "             # The patch replaces the function temporarily within this block\n",
    "             result_correct = validate_claim_with_llm(dummy_claim, dummy_context)\n",
    "\n",
    "        assert result_correct == {\"Resultado\": \"Correta\", \"Justificativa\": \"N/A\"}, f\"Parsing failed for Correct case, got {result_correct}\"\n",
    "        print(f\"Parsed Result: {result_correct}\")\n",
    "        print(\"-> Subtest 1 PASSED\")\n",
    "    except Exception as e:\n",
    "         print(f\"-> Subtest 1 FAILED: {e}\")\n",
    "         all_mock_tests_passed = False\n",
    "\n",
    "\n",
    "    # --- Subtest 2: Mock Incorrect Response ---\n",
    "    print(\"\\n--- Subtest 2: Mocking Incorrect Response ---\")\n",
    "    mock_response_incorrect = \"Resultado: Incorreta\\nJustificativa: O trecho 1 contradiz a alegação.\"\n",
    "    mock_tokenizer.decode.return_value = mock_response_incorrect\n",
    "    try:\n",
    "        with patch('src.validator.load_llm_model_and_tokenizer', return_value=(mock_model, mock_tokenizer)):\n",
    "             result_incorrect = validate_claim_with_llm(dummy_claim, dummy_context)\n",
    "\n",
    "        expected_incorrect = {\"Resultado\": \"Incorreta\", \"Justificativa\": \"O trecho 1 contradiz a alegação.\"}\n",
    "        assert result_incorrect == expected_incorrect, f\"Parsing failed for Incorrect case, got {result_incorrect}\"\n",
    "        print(f\"Parsed Result: {result_incorrect}\")\n",
    "        print(\"-> Subtest 2 PASSED\")\n",
    "    except Exception as e:\n",
    "         print(f\"-> Subtest 2 FAILED: {e}\")\n",
    "         all_mock_tests_passed = False\n",
    "\n",
    "\n",
    "    # --- Subtest 3: Mock Malformed Response ---\n",
    "    print(\"\\n--- Subtest 3: Mocking Malformed Response ---\")\n",
    "    mock_response_malformed = \"Uh oh, I forgot the format.\\nResultado : maybe correct\\nJustif: idk\"\n",
    "    mock_tokenizer.decode.return_value = mock_response_malformed\n",
    "    try:\n",
    "        with patch('src.validator.load_llm_model_and_tokenizer', return_value=(mock_model, mock_tokenizer)):\n",
    "             result_malformed = validate_claim_with_llm(dummy_claim, dummy_context)\n",
    "\n",
    "        # Expect the default error state\n",
    "        expected_malformed = {\"Resultado\": \"Erro\", \"Justificativa\": \"Falha ao parsear resposta do LLM.\"}\n",
    "        assert result_malformed == expected_malformed, f\"Parsing failed for Malformed case, got {result_malformed}\"\n",
    "        print(f\"Parsed Result: {result_malformed}\")\n",
    "        print(\"-> Subtest 3 PASSED\")\n",
    "    except Exception as e:\n",
    "         print(f\"-> Subtest 3 FAILED: {e}\")\n",
    "         all_mock_tests_passed = False\n",
    "\n",
    "    test_passed = all_mock_tests_passed\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*10 + \" End of Test: test_llm_parsing (Mocked) \" + \"=\"*10)\n",
    "if test_passed:\n",
    "    print(\"Result: test_llm_parsing PASSED\")\n",
    "else:\n",
    "    print(\"Result: test_llm_parsing FAILED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acordao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
