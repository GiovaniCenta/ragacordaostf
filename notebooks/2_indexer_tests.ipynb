{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding f:\\interview\\acordao\\acordao_validator to sys.path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import shutil # For cleaning up the test database directory\n",
    "import time\n",
    "\n",
    "# Configure logging for tests\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - TEST - %(levelname)s - %(message)s')\n",
    "\n",
    "# Add the 'src' directory to the Python path to find the modules\n",
    "module_path = os.path.abspath(os.path.join('..')) # Assumes notebook is in 'notebooks/' dir\n",
    "if module_path not in sys.path:\n",
    "    print(f\"Adding {module_path} to sys.path\")\n",
    "    sys.path.append(module_path)\n",
    "else:\n",
    "    print(f\"{module_path} already in sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported from data_loader.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\interview\\acordao\\acordao_validator\\acordao\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported from indexer.\n",
      "\n",
      "Setup complete. Using test ChromaDB directory: f:\\interview\\acordao\\acordao_validator\\notebooks\\chroma_db_index\n",
      "Cleaning up existing test ChromaDB directory: f:\\interview\\acordao\\acordao_validator\\notebooks\\chroma_db_index\n",
      "Cleanup successful.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import functions from our modules\n",
    "try:\n",
    "    from src.data_loader import load_and_prepare_data\n",
    "    print(\"Successfully imported from data_loader.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR importing from data_loader: {e}\")\n",
    "    load_and_prepare_data = None\n",
    "\n",
    "try:\n",
    "    from src.indexer import get_embedding_model, create_or_update_index\n",
    "    # Also import constants used by the indexer if needed for verification\n",
    "    from src.indexer import CHROMA_PERSIST_DIR, CHROMA_COLLECTION_NAME, EMBEDDING_MODEL_NAME\n",
    "    print(\"Successfully imported from indexer.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR importing from indexer: {e}\")\n",
    "    get_embedding_model = None\n",
    "    create_or_update_index = None\n",
    "    # Define defaults if import fails to avoid errors later\n",
    "    CHROMA_PERSIST_DIR = os.path.join(\"..\", \"chroma_db_test_index\") # Use a separate test dir\n",
    "    CHROMA_COLLECTION_NAME = \"test_acordaos\"\n",
    "    EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-large-instruct\" # Keep consistent\n",
    "\n",
    "\n",
    "# --- Helper Function for Cleanup ---\n",
    "def cleanup_chroma_test_db(persist_dir=CHROMA_PERSIST_DIR):\n",
    "    \"\"\"Removes the ChromaDB test directory if it exists.\"\"\"\n",
    "    abs_persist_dir = os.path.abspath(persist_dir) # Use absolute path\n",
    "    if os.path.exists(abs_persist_dir):\n",
    "        print(f\"Cleaning up existing test ChromaDB directory: {abs_persist_dir}\")\n",
    "        try:\n",
    "            shutil.rmtree(abs_persist_dir)\n",
    "            print(\"Cleanup successful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during cleanup: {e}. Manual deletion might be required.\")\n",
    "    else:\n",
    "        print(f\"Test ChromaDB directory not found (no cleanup needed): {abs_persist_dir}\")\n",
    "\n",
    "print(f\"\\nSetup complete. Using test ChromaDB directory: {os.path.abspath(CHROMA_PERSIST_DIR)}\")\n",
    "# Initial cleanup before tests start\n",
    "cleanup_chroma_test_db(CHROMA_PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 14:41:51,626 - TEST - INFO - Loading embedding model 'intfloat/multilingual-e5-large-instruct' onto device: cuda\n",
      "2025-04-30 14:41:51,629 - TEST - INFO - Load pretrained SentenceTransformer: intfloat/multilingual-e5-large-instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported from indexer.\n",
      "\n",
      "========== Running test: test_model_loading ==========\n",
      "Attempting to load model: intfloat/multilingual-e5-large-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 14:42:03,404 - TEST - INFO - Embedding model 'intfloat/multilingual-e5-large-instruct' loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 11.90 seconds.\n",
      "Model class: <class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "Model loaded successfully on device: cuda:0\n",
      "-> test_model_loading PASSED\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Test Embedding Model Loading\n",
    "try:\n",
    "    from src.indexer import get_embedding_model, create_or_update_index\n",
    "    # Also import constants used by the indexer if needed for verification\n",
    "    from src.indexer import CHROMA_PERSIST_DIR, CHROMA_COLLECTION_NAME, EMBEDDING_MODEL_NAME\n",
    "    print(\"Successfully imported from indexer.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR importing from indexer: {e}\")\n",
    "    get_embedding_model = None # This is why the check `if get_embedding_model is None:` exists in the test\n",
    "    # ... other fallback definitions ...\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import time # Make sure time is imported if it wasn't already in this cell scope\n",
    "\n",
    "def test_model_loading():\n",
    "    \"\"\"Tests the get_embedding_model function.\"\"\"\n",
    "    if get_embedding_model is None:\n",
    "        print(\"Skipping test_model_loading due to import error.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*10 + \" Running test: test_model_loading \" + \"=\"*10)\n",
    "    start_time = time.time()\n",
    "    model = None\n",
    "    try:\n",
    "        # Use the model name defined in the indexer constants\n",
    "        print(f\"Attempting to load model: {EMBEDDING_MODEL_NAME}\")\n",
    "        model = get_embedding_model() # Uses the default defined in indexer\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"Model loaded in {load_time:.2f} seconds.\")\n",
    "\n",
    "        # Assertions\n",
    "        assert model is not None, \"Model loading returned None\"\n",
    "        assert isinstance(model, SentenceTransformer), f\"Expected SentenceTransformer, got {type(model)}\"\n",
    "        print(f\"Model class: {type(model)}\")\n",
    "        # Check the device (requires torch)\n",
    "        expected_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Note: model.device might be a torch.device object\n",
    "        # Check the device type instead of the exact string 'cuda:0'\n",
    "        assert model.device.type == expected_device, f\"Model loaded on unexpected device type: {model.device.type} (Expected: {expected_device})\"\n",
    "        print(f\"Model loaded successfully on device: {model.device}\")\n",
    "        print(\"-> test_model_loading PASSED\")\n",
    "        return model # Return model for use in next test\n",
    "\n",
    "    except Exception as e:\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"-> test_model_loading FAILED after {load_time:.2f} seconds: {e}\")\n",
    "        # Re-raise if you want the notebook execution to stop on failure\n",
    "        # raise e\n",
    "        return None # Return None if failed\n",
    "\n",
    "\n",
    "# --- Run the Test ---\n",
    "# This might take a while the first time it downloads the model\n",
    "embedding_model_instance = test_model_loading()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 14:42:03,464 - TEST - INFO - Starting data loading and preparation for:\n",
      "2025-04-30 14:42:03,466 - TEST - INFO -   Acordão: f:\\interview\\acordao\\acordao_validator\\data\\Acórdão 733 de 2025 Plenário.pdf\n",
      "2025-04-30 14:42:03,467 - TEST - INFO -   Resumo: f:\\interview\\acordao\\acordao_validator\\data\\Acórdão 733-2025 resumos.txt\n",
      "2025-04-30 14:42:03,497 - TEST - INFO - Reading PDF: f:\\interview\\acordao\\acordao_validator\\data\\Acórdão 733 de 2025 Plenário.pdf with 44 pages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ChromaDB directory not found (no cleanup needed): f:\\interview\\acordao\\acordao_validator\\notebooks\\chroma_db_index\n",
      "\n",
      "========== Running test: test_indexing ==========\n",
      "\n",
      "Loading data using data_loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 14:42:04,119 - TEST - INFO - Processing Acordão as PDF file.\n",
      "2025-04-30 14:42:04,121 - TEST - INFO - Processed 44 chunks from the acórdão.\n",
      "2025-04-30 14:42:04,121 - TEST - INFO - Processed 3 claims from the resumo.\n",
      "2025-04-30 14:42:04,122 - TEST - INFO - Starting index creation/update process for 44 chunks...\n",
      "2025-04-30 14:42:04,122 - TEST - INFO - Initializing ChromaDB client at path: .\\chroma_db_index\n",
      "2025-04-30 14:42:04,242 - TEST - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44 acórdão chunks.\n",
      "\n",
      "Running create_or_update_index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 14:42:04,541 - TEST - INFO - Getting or creating collection: acordaos\n",
      "2025-04-30 14:42:04,604 - TEST - INFO - Preparing data for indexing (IDs, prefixed text, metadata)...\n",
      "2025-04-30 14:42:04,605 - TEST - INFO - Generating embeddings for 44 chunks using model intfloat/multilingual-e5-large-instruct...\n",
      "Batches: 100%|██████████| 2/2 [00:06<00:00,  3.47s/it]\n",
      "2025-04-30 14:42:11,586 - TEST - INFO - Embedding generation complete.\n",
      "2025-04-30 14:42:11,587 - TEST - INFO - Upserting 44 items into ChromaDB collection 'acordaos'...\n",
      "2025-04-30 14:42:11,921 - TEST - INFO - Upsert completed successfully.\n",
      "2025-04-30 14:42:11,924 - TEST - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing function completed in 7.80 seconds.\n",
      "\n",
      "Verifying results...\n",
      "  ChromaDB directory found: f:\\interview\\acordao\\acordao_validator\\notebooks\\chroma_db_index\n",
      "  Attempting to connect to persistent client at f:\\interview\\acordao\\acordao_validator\\notebooks\\chroma_db_index...\n",
      "  Attempting to get collection: acordaos\n",
      "  Collection count: 44\n",
      "  Collection count matches loaded chunks.\n",
      "  Peeking at one item...\n",
      "  Peek successful. ID: Acórdão 733 de 2025 Plenário.pdf_0, Metadata: {'chunk_index': 0, 'chunk_type': 'paragraph', 'page_number': 1, 'source': 'f:\\\\interview\\\\acordao\\\\acordao_validator\\\\data\\\\Acórdão 733 de 2025 Plenário.pdf'}\n",
      "\n",
      "========== End of test: test_indexing ==========\n",
      "Result: TEST PASSED\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test Index Creation/Update\n",
    "\n",
    "import chromadb # Need this for verification step\n",
    "\n",
    "def test_indexing(base_dir, model):\n",
    "    \"\"\"Tests the create_or_update_index function.\"\"\"\n",
    "    if create_or_update_index is None or load_and_prepare_data is None:\n",
    "        print(\"Skipping test_indexing due to import errors in Cell 1.\")\n",
    "        return\n",
    "    if model is None:\n",
    "        print(\"Skipping test_indexing because embedding model failed to load in Cell 2.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*10 + \" Running test: test_indexing \" + \"=\"*10)\n",
    "    data_dir = os.path.join(base_dir, \"data\")\n",
    "    persist_dir = os.path.abspath(CHROMA_PERSIST_DIR) # Use absolute path\n",
    "\n",
    "    # --- Test Files ---\n",
    "    acordao_file = \"Acórdão 733 de 2025 Plenário.pdf\" # Use a real file\n",
    "    resumo_file = \"Acórdão 733-2025 resumos.txt\"\n",
    "    acordao_path = os.path.join(data_dir, acordao_file)\n",
    "    resumo_path = os.path.join(data_dir, resumo_file)\n",
    "\n",
    "    all_passed = True\n",
    "\n",
    "    # --- Setup: Load data ---\n",
    "    print(f\"\\nLoading data using data_loader...\")\n",
    "    acordao_chunks, _ = load_and_prepare_data(acordao_path, resumo_path)\n",
    "    if acordao_chunks is None:\n",
    "        print(\"-> Setup FAILED: data_loader failed to load data.\")\n",
    "        return # Cannot proceed\n",
    "\n",
    "    num_chunks_loaded = len(acordao_chunks)\n",
    "    print(f\"Loaded {num_chunks_loaded} acórdão chunks.\")\n",
    "    if num_chunks_loaded == 0:\n",
    "        print(\"Warning: No chunks loaded, indexing test will be trivial.\")\n",
    "\n",
    "    # --- Execution: Run the indexer ---\n",
    "    print(f\"\\nRunning create_or_update_index...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        create_or_update_index(acordao_chunks, model)\n",
    "        index_time = time.time() - start_time\n",
    "        print(f\"Indexing function completed in {index_time:.2f} seconds.\")\n",
    "    except Exception as e:\n",
    "        index_time = time.time() - start_time\n",
    "        print(f\"-> Indexing FAILED after {index_time:.2f} seconds: {e}\")\n",
    "        # Optional: Re-raise if needed\n",
    "        # raise e\n",
    "        all_passed = False\n",
    "        # Attempt verification anyway, maybe the directory was created\n",
    "        pass\n",
    "\n",
    "    # --- Verification ---\n",
    "    print(f\"\\nVerifying results...\")\n",
    "    # 1. Check if persist directory exists\n",
    "    if not os.path.exists(persist_dir):\n",
    "        print(f\"-> Verification FAILED: ChromaDB directory was not created at {persist_dir}\")\n",
    "        all_passed = False\n",
    "    else:\n",
    "        print(f\"  ChromaDB directory found: {persist_dir}\")\n",
    "\n",
    "        # 2. Try to connect and check collection count\n",
    "        try:\n",
    "            print(f\"  Attempting to connect to persistent client at {persist_dir}...\")\n",
    "            verify_client = chromadb.PersistentClient(path=persist_dir)\n",
    "            print(f\"  Attempting to get collection: {CHROMA_COLLECTION_NAME}\")\n",
    "            verify_collection = verify_client.get_collection(name=CHROMA_COLLECTION_NAME)\n",
    "            collection_count = verify_collection.count()\n",
    "            print(f\"  Collection count: {collection_count}\")\n",
    "\n",
    "            # Assert count matches number of chunks loaded\n",
    "            if collection_count != num_chunks_loaded:\n",
    "                 print(f\"-> Verification FAILED: Collection count ({collection_count}) does not match loaded chunks ({num_chunks_loaded})\")\n",
    "                 all_passed = False\n",
    "            else:\n",
    "                 print(\"  Collection count matches loaded chunks.\")\n",
    "\n",
    "                 # 3. (Optional) Peek at one item\n",
    "                 if collection_count > 0:\n",
    "                    print(\"  Peeking at one item...\")\n",
    "                    peek_result = verify_collection.peek(limit=1)\n",
    "                    # Check if necessary keys are present in the peek result\n",
    "                    if not peek_result or not peek_result.get('ids') or not peek_result.get('documents') or not peek_result.get('metadatas'):\n",
    "                         print(f\"-> Verification FAILED: Peek result structure is incorrect: {peek_result}\")\n",
    "                         all_passed = False\n",
    "                    else:\n",
    "                         print(f\"  Peek successful. ID: {peek_result['ids'][0]}, Metadata: {peek_result['metadatas'][0]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"-> Verification FAILED: Error during ChromaDB verification: {e}\")\n",
    "            all_passed = False\n",
    "\n",
    "    print(\"\\n\" + \"=\"*10 + \" End of test: test_indexing \" + \"=\"*10)\n",
    "    if all_passed:\n",
    "        print(\"Result: TEST PASSED\")\n",
    "    else:\n",
    "        print(\"Result: TEST FAILED\")\n",
    "\n",
    "# --- Run the Test ---\n",
    "base_project_dir = os.path.abspath(os.path.join('..'))\n",
    "# Run cleanup before the test\n",
    "cleanup_chroma_test_db(CHROMA_PERSIST_DIR)\n",
    "# Pass the loaded model from Cell 2\n",
    "test_indexing(base_project_dir, embedding_model_instance)\n",
    "# Optional: Run cleanup again after the test\n",
    "# cleanup_chroma_test_db(CHROMA_PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acordao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
